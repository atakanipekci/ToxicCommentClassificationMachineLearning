{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import model_from_json\n",
    "  \n",
    "df = pd.read_csv('train.csv')\n",
    "train=df\n",
    "#train.insert(2,\"clean\",train['toxic']^1)\n",
    "train_comments = train['comment_text']\n",
    "train_toxic_comments=df.loc[train['toxic'] == 1]\n",
    "df= pd.read_csv('test.csv')\n",
    "test=df\n",
    "\n",
    "test_comments=df['comment_text']\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df2 = pd.read_csv('turkce_yorumlar.csv')\n",
    "df2.head()\n",
    "df2.describe()\n",
    "train2=df2\n",
    "#train.insert(2,\"clean\",train['toxic']^1)\n",
    "train_comments2 = train2['yorum']\n",
    "y2= train2[\"toxic\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,SpatialDropout1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D,GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "max_features = 100000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_comments) + list(test_comments))\n",
    "train_tokenized = tokenizer.texts_to_sequences(train_comments)\n",
    "test_tokenized = tokenizer.texts_to_sequences(test_comments)\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "maxlen = 150\n",
    "X_t = pad_sequences(train_tokenized, maxlen=maxlen)\n",
    "X_te = pad_sequences(test_tokenized, maxlen=maxlen)\n",
    "inp = Input(shape=(maxlen, ))\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(\"glove.840B.300d.txt\", encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fitting...\n",
      "Epoch 1/1\n",
      "159571/159571 [==============================] - 5076s 32ms/step - loss: 0.0501 - acc: 0.9815\n",
      "start predicting...\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x1 = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "x2 = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "conc = concatenate([x1, x2])\n",
    "#x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.15, recurrent_dropout=0.15))(x)\n",
    "#x = Conv1D(64, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform')(x)\n",
    "#x=GlobalAveragePooling1D()(x)\n",
    "avg_pool = GlobalAveragePooling1D()(conc)\n",
    "max_pool = GlobalMaxPooling1D()(conc)\n",
    "conc = concatenate([avg_pool, max_pool])\n",
    "x = Dense(64, activation='relu')(conc)\n",
    "x = Dropout(0.2)(x)\n",
    "out = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "#out = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(\"start fitting...\")\n",
    "model.fit(X_t,y, epochs=1, batch_size=32, verbose =1)\n",
    "\n",
    "print(\"start predicting...\")\n",
    "y_pred = model.predict(X_te, batch_size=1024)\n",
    "\n",
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "for idx, col in enumerate(list_classes):\n",
    "    submission[col] = y_pred[:,idx]\n",
    "submission.to_csv('submission_lstm_glove.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    max_df=0.7,\n",
    "    ngram_range=(1,1),\n",
    "    )\n",
    "word_vectorizer.fit(train_comments2)\n",
    "X_t2=word_vectorizer.transform(train_comments2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "clf = joblib.load('turkish_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bütünüyle eksiksiz ve harika\n",
      "Eğlenceli zaman geçirmek için iyi bir yöntem\n",
      "Bu oyunu oynamadan ölmeyin!\n",
      "İndirimde bu oyunu kaçırmayın kaçırtmayın\n",
      "oyun bence çok güzel fakat online arkadaşlarınla oynana bilseydi dahada güzel olurdu\n",
      "Ne kadar oynasanız da sıkmayan bir oyun\n",
      "oyun çok eğlenceli kesinlikle tavsiye ediyorum\n",
      "BEN NEDEN PARA VERDIM OROSPU ÇOCUKLARI\n",
      "The photos seem to suggest one point of view (that Asbury Park's a piece of crap [pardon the French]) and the article suggests another (Asbury Park: Gettin' better every day).  Thoughts?\n",
      "== Leona Lews... == \n",
      "\n",
      " ...is a slag. \n",
      "\n",
      " I Abi Branning frankly since she's pure and innocent whereas Leona is a whore. Anuway I want to cuddle up with Lorna Fitzgerald as she's so sweet and cute and magical.\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "window = Tk()\n",
    "window.title(\"Toxical Analysis\")\n",
    "window.geometry('1280x720')\n",
    "#window.config(bg=\"#CCEEFF\")\n",
    "selected = IntVar()\n",
    "rad1 = Radiobutton(window,text='Turkish', value=1, variable=selected)\n",
    "rad1.pack(side=tk.RIGHT)\n",
    "rad2 = Radiobutton(window,text='English', value=2, variable=selected)\n",
    "rad2.pack(side=tk.RIGHT)\n",
    "\n",
    "lbl = Label(window, text=\"    Enter Text For Toxical Analysis      \") \n",
    "lbl.pack()\n",
    "\n",
    "txt = scrolledtext.ScrolledText(window,height=10, width=100)\n",
    "\n",
    "txt.pack()\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#background_image=tk.PhotoImage(file=\"background.png\")\n",
    "#background_label = tk.Label(window, image=background_image)\n",
    "#background_label.place(x=0, y=0, relwidth=1, relheight=1)\n",
    "#figure1 = Figure(figsize=(5,4), dpi=100) \n",
    "#subplot1 = figure1.add_subplot(111) \n",
    "\n",
    "def clicked(): \n",
    "    \n",
    "    test=txt.get(\"1.0\",\"end-1c\")\n",
    "    df=pd.DataFrame([str(test)],columns=['comment'])\n",
    "    comment=df['comment']\n",
    "    \n",
    "    if(selected.get()==1):\n",
    "        vect=word_vectorizer.transform(comment)\n",
    "        #p=clf.predict_proba(vect)[0][1]\n",
    "        p=clf.predict(vect)\n",
    "        result=\"Toxic= \"+str(p)\n",
    "    elif(selected.get()==2):\n",
    "        vect=tokenizer.texts_to_sequences(comment)\n",
    "        vect=pad_sequences(vect, maxlen=maxlen)\n",
    "        #print(vect)\n",
    "\n",
    "        predictions=loaded_model.predict(vect)\n",
    "        result=\"\\n\\nToxic= \"+str(predictions[0][0])+\"\\nSevere Toxic= \"+str(predictions[0][1])\n",
    "        result+=\"\\nObscene= \"+str(predictions[0][2])+\"\\nThreat= \"+str(predictions[0][3])+\"\\nInsult= \"+str(predictions[0][4])\n",
    "        result+=\"\\nIdentity Hate= \"+str(predictions[0][5])\n",
    "    else:\n",
    "        result=\"Please Select A Language\"\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #objects = ('Toxic', 'Severe Toxic', 'Obscene', 'Threat', 'Insult', 'Identity Hate')\n",
    "    #y_pos = np.arange(len(objects))\n",
    "    #subplot1.bar(objects,predictions[0], color = 'g') \n",
    "    #bar1 = FigureCanvasTkAgg(figure1, window) \n",
    "    #bar1.get_tk_widget().pack(side=tk.LEFT, fill=tk.BOTH, expand=0)\n",
    "    #print(predictions[0])\n",
    "    \n",
    "    lbl2.configure(text=result)\n",
    "    \n",
    "    \n",
    "def clicked2(): \n",
    "    \n",
    "    \n",
    "    \n",
    "    if(selected.get()==1):\n",
    "        random_text=df2.sample()[\"yorum\"].values[0]\n",
    "        print(random_text)\n",
    "        txt.delete('1.0', END)\n",
    "        txt.insert(INSERT, random_text)\n",
    "    elif(selected.get()==2):\n",
    "        random_text=df.sample()[\"comment_text\"].values[0]\n",
    "        print(random_text)\n",
    "        txt.delete('1.0', END)\n",
    "        txt.insert(INSERT, random_text)\n",
    "    else:\n",
    "        lbl2.configure(text=\"Please Select A Language\")\n",
    "\n",
    "btn = Button(window, text=\"Start The Analysis\", command=clicked,background = 'black', foreground = \"white\")\n",
    "btn.pack()\n",
    "btn2 = Button(window, text=\"Add Random Comment\", command=clicked2,background = 'black', foreground = \"white\")\n",
    "btn2.pack()\n",
    "lbl2 = Label(window, text=\"*\",justify=tk.LEFT) \n",
    "lbl2.pack(side=tk.LEFT)\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbconvert --to script config_template.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['turkish_model.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "classifier = LogisticRegression(class_weight='balanced')\n",
    "classifier.fit(X_t2,y2)\n",
    "joblib.dump(classifier, 'turkish_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = joblib.load('turkish_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
